{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a038d9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spark.apache.org/docs/latest/quick-start.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaa004d",
   "metadata": {},
   "source": [
    "# Interactive analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f792cb",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0e1457b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-20T15:41:22.743360Z",
     "start_time": "2022-11-20T15:41:15.559468Z"
    }
   },
   "outputs": [],
   "source": [
    "textFile = spark.read.text(\"README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caadc0fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-20T15:41:52.292195Z",
     "start_time": "2022-11-20T15:41:52.060693Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Assignment 3: Apache Spark\r\n",
      "\r\n",
      "The goal of this assignment is to learn how to do large-scale data analysis tasks using Apache Spark: for this assignment, we will use relatively small datasets and  we won't run anything in distributed mode; however Spark can be easily used to run the same programs on much larger datasets.\r\n",
      "\r\n",
      "### Getting Started with Spark\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68f56592",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-20T15:42:52.098581Z",
     "start_time": "2022-11-20T15:42:52.091768Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[value: string]\n",
      "DataFrame[value: string]\n"
     ]
    }
   ],
   "source": [
    "print(textFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2de8d75e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-20T15:43:23.888208Z",
     "start_time": "2022-11-20T15:43:23.710571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|# Assignment 3: A...|\n",
      "|                    |\n",
      "|The goal of this ...|\n",
      "|                    |\n",
      "|### Getting Start...|\n",
      "|                    |\n",
      "|This guide is bas...|\n",
      "|                    |\n",
      "|[Apache Spark](ht...|\n",
      "|                    |\n",
      "|Spark can be used...|\n",
      "|                    |\n",
      "|### Installing Spark|\n",
      "|                    |\n",
      "|Since the Spark d...|\n",
      "|                    |\n",
      "|1. Download the S...|\n",
      "|2. Move the downl...|\n",
      "|`tar zxvf spark-3...|\n",
      "|3. This will crea...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textFile.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47ded47f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-20T15:41:37.755925Z",
     "start_time": "2022-11-20T15:41:35.430844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n"
     ]
    }
   ],
   "source": [
    "# Print number of rows.\n",
    "print(textFile.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4078797",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-20T15:43:15.188030Z",
     "start_time": "2022-11-20T15:43:15.055417Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='# Assignment 3: Apache Spark'),\n",
       " Row(value=''),\n",
       " Row(value=\"The goal of this assignment is to learn how to do large-scale data analysis tasks using Apache Spark: for this assignment, we will use relatively small datasets and  we won't run anything in distributed mode; however Spark can be easily used to run the same programs on much larger datasets.\"),\n",
       " Row(value=''),\n",
       " Row(value='### Getting Started with Spark'),\n",
       " Row(value=''),\n",
       " Row(value='This guide is basically a summary of the excellent tutorials that can be found at the [Spark website](http://spark.apache.org).'),\n",
       " Row(value=''),\n",
       " Row(value='[Apache Spark](https://spark.apache.org) is a relatively new cluster computing framework, developed originally at UC Berkeley. It significantly generalizes the 2-stage Map-Reduce paradigm (originally proposed by Google and popularized by open-source Hadoop system); Spark is instead based on the abstraction of **resilient distributed datasets (RDDs)**. An RDD is basically a distributed collection of items, that can be created in a variety of ways. Spark provides a set of operations to transform one or more RDDs into an output RDD, and analysis tasks are written as chains of these operations.'),\n",
       " Row(value=''),\n",
       " Row(value='Spark can be used with the Hadoop ecosystem, including the HDFS file system and the YARN resource manager. '),\n",
       " Row(value=''),\n",
       " Row(value='### Installing Spark'),\n",
       " Row(value=''),\n",
       " Row(value='Since the Spark distribution is large, we ask you to download that directly from the Spark website.'),\n",
       " Row(value=''),\n",
       " Row(value='1. Download the Spark package at http://spark.apache.org/downloads.html. We will use **Version 3.2.0, Pre-built for Hadoop 3.3 or later**.'),\n",
       " Row(value=\"2. Move the downloaded file to the `Assignment-3/` directory (so it is available in '/data/Assignment-3'), and uncompress it using: \"),\n",
       " Row(value='`tar zxvf spark-3.2.0-bin-hadoop3.2.tgz`'),\n",
       " Row(value='3. This will create a new directory: `spark-3.2.0-bin-hadoop3.2/`. '),\n",
       " Row(value='4. Set the SPARKHOME variable: `export SPARKHOME=/data/Assignment-3/spark-3.2.0-bin-hadoop3.2/` (modify appropriately if it is downloaded somewhere else).'),\n",
       " Row(value=''),\n",
       " Row(value='We are ready to use Spark. '),\n",
       " Row(value=''),\n",
       " Row(value='### Spark and Python'),\n",
       " Row(value=''),\n",
       " Row(value='Spark primarily supports three languages: Scala (Spark is written in Scala), Java, and Python. We will use Python here -- you can follow the instructions at the tutorial'),\n",
       " Row(value='and quick start (http://spark.apache.org/docs/latest/quick-start.html) for other languages. The Java equivalent code can be very verbose and hard to follow. The below'),\n",
       " Row(value='shows a way to use the Python interface through the standard Python shell.'),\n",
       " Row(value=''),\n",
       " Row(value='### Jupyter Notebook'),\n",
       " Row(value=''),\n",
       " Row(value='To use Spark within the Jupyter Notebook (and to play with the Notebook we have provided), you can do:'),\n",
       " Row(value='\\t```'),\n",
       " Row(value='\\tPYSPARK_PYTHON=/usr/bin/python3 PYSPARK_DRIVER_PYTHON=\"jupyter\" PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --allow-root --no-browser --ip=0.0.0.0 --port=8881\" $SPARKHOME/bin/pyspark'),\n",
       " Row(value='\\t```'),\n",
       " Row(value='You need to make sure you are mapping the port 8881 for this to work.'),\n",
       " Row(value=''),\n",
       " Row(value='### PySpark Shell'),\n",
       " Row(value=''),\n",
       " Row(value='You can also use the PySpark Shell directly.'),\n",
       " Row(value=''),\n",
       " Row(value='1. `$SPARKHOME/bin/pyspark`: This will start a Python shell (it will also output a bunch of stuff about what Spark is doing). The relevant variables are initialized in this python'),\n",
       " Row(value='shell, but otherwise it is just a standard Python shell.'),\n",
       " Row(value=''),\n",
       " Row(value='2. `>>> textFile = sc.textFile(\"README.md\")`: This creates a new RDD, called `textFile`, by reading data from a local file. The `sc.textFile` commands create an RDD'),\n",
       " Row(value='containing one entry per line in the file.'),\n",
       " Row(value=''),\n",
       " Row(value='3. You can see some information about the RDD by doing `textFile.count()` or `textFile.first()`, or `textFile.take(5)` (which prints an array containing 5 items from the RDD).'),\n",
       " Row(value=''),\n",
       " Row(value='4. We recommend you follow the rest of the commands in the quick start guide (http://spark.apache.org/docs/latest/quick-start.html). Here we will simply do the Word Count'),\n",
       " Row(value='application.'),\n",
       " Row(value=''),\n",
       " Row(value='#### Word Count Application'),\n",
       " Row(value=''),\n",
       " Row(value='The following command (in the pyspark shell) does a word count, i.e., it counts the number of times each word appears in the file `README.md`. Use `counts.take(5)` to see the output.'),\n",
       " Row(value=''),\n",
       " Row(value='`>>> counts = textFile.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)`'),\n",
       " Row(value=''),\n",
       " Row(value='Here is the same code without the use of `lambda` functions.'),\n",
       " Row(value=''),\n",
       " Row(value='```'),\n",
       " Row(value='def split(line): '),\n",
       " Row(value='    return line.split(\" \")'),\n",
       " Row(value='def generateone(word): '),\n",
       " Row(value='    return (word, 1)'),\n",
       " Row(value='def sum(a, b):'),\n",
       " Row(value='    return a + b'),\n",
       " Row(value=''),\n",
       " Row(value='textfile.flatMap(split).map(generateone).reduceByKey(sum)'),\n",
       " Row(value='```'),\n",
       " Row(value=''),\n",
       " Row(value='The `flatmap` splits each line into words, and the following `map` and `reduce` do the counting (we will discuss this in the class, but here is an excellent and detailed'),\n",
       " Row(value='description: [Hadoop Map-Reduce Tutorial](http://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html#Source+Code) (look for Walk-Through).'),\n",
       " Row(value=''),\n",
       " Row(value='The `lambda` representation is more compact and preferable, especially for small functions, but for large functions, it is better to separate out the definitions.'),\n",
       " Row(value=''),\n",
       " Row(value='### Running it as an Application'),\n",
       " Row(value=''),\n",
       " Row(value='Instead of using a shell, you can also write your code as a python file, and *submit* that to the spark cluster. The `project5` directory contains a python file `wordcount.py`,'),\n",
       " Row(value='which runs the program in a local mode. To run the program, do:'),\n",
       " Row(value='`$SPARKHOME/bin/spark-submit wordcount.py`'),\n",
       " Row(value=''),\n",
       " Row(value='### More...'),\n",
       " Row(value=''),\n",
       " Row(value='We encourage you to look at the [Spark Programming Guide](https://spark.apache.org/docs/latest/programming-guide.html) and play with the other RDD manipulation commands. '),\n",
       " Row(value='You should also try out the Scala and Java interfaces.'),\n",
       " Row(value=''),\n",
       " Row(value='### Assignment Details'),\n",
       " Row(value=''),\n",
       " Row(value='We have provided a Python file: `spark_assignment.py`, that initializes the folllowing RDDs:'),\n",
       " Row(value='* An RDD consisting of lines from a Shakespeare play (`play.txt`)'),\n",
       " Row(value='* An RDD consisting of lines from a log file (`NASA_logs_sample.txt`)'),\n",
       " Row(value='* An RDD consisting of 2-tuples indicating user-product ratings from Amazon Dataset (`amazon-ratings.txt`)'),\n",
       " Row(value='* An RDD consisting of JSON documents pertaining to all the Noble Laureates over last few years (`prize.json`)'),\n",
       " Row(value=''),\n",
       " Row(value='The file also contains some examples of operations on these RDDs. '),\n",
       " Row(value=''),\n",
       " Row(value='Your tasks are to fill out the 8 functions that are defined in the `functions.py` file (starting with `task`). The amount of code that you '),\n",
       " Row(value='write would typically be small (several would be one-liners), with the exception of the last one. '),\n",
       " Row(value=''),\n",
       " Row(value='- **Task 1**: Write the function that takes as input the `amazonInputRDD` (which is an RDD of lines) and'),\n",
       " Row(value='`maps` each line to a tuple while removing the initial descriptor, i.e., the first line \"user1 product1 5.0\" gets mapped to a tuple `(1, 1, 5.0)`. This just requires a single `map`.'),\n",
       " Row(value=''),\n",
       " Row(value='- **Task 2**: Complete the function that takes as input the `amazonInputRDD` and computes the'),\n",
       " Row(value='average rating for each user across all the products they reviewed. '),\n",
       " Row(value='The output should be an RDD of 2-tuples of the form `(user1, 2.87)` (not the correct answer).'),\n",
       " Row(value='You can either use `aggregateByKey` or a `reduceByKey` followed by a `map`.'),\n",
       " Row(value=''),\n",
       " Row(value='- **Task 3**: Complete the function that takes as input the `amazonInputRDD` and computes the'),\n",
       " Row(value='`mode` rating for each product across all users (i.e., the rating that was most common for that'),\n",
       " Row(value='product). If there are ties, pick the higher rating. Easiest way to do this would be a'),\n",
       " Row(value='`groupByKey` followed by a map to compute the `mode`.'),\n",
       " Row(value=''),\n",
       " Row(value='- **Task 4**: For `logsRDD`, write a function that computes the number of log requests for each year. So the output should be an RDD with records of'),\n",
       " Row(value='teh form `(1995, 2952)` (not the correct answer). This can be done through a `map` to extract the years, followed by a group by aggregate.'),\n",
       " Row(value=''),\n",
       " Row(value='- **Task 5**: Write just the flatmap function `task5_flatmap` that operates on `playRDD` -- for each line, it outputs the individual words sanitized'),\n",
       " Row(value='to remove any non-alphanumerical characters. So for the 3rd line, it would output a list: `[Enter, LEONATO, HERO, and, BEATRICE, with, a, Messenger]`.'),\n",
       " Row(value=''),\n",
       " Row(value='- **Task 6**: This takes as input the playRDD and for each line, finds the first word in the line, and also counts the number of words. It should then filter the RDD by only selecting the lines where the count of words in the line is > 10. The output will be an RDD where the key is the first word in the line, and the value is a 2-tuple, the first being the line and the second being the number of words (which must be >10). Simplest way to do it is probably a `map` followed by a `filter`.'),\n",
       " Row(value=''),\n",
       " Row(value='- **Task 7**: Write just the flatmap function (`task7_flatmap`) that takes in a parsed JSON document (from `prize.json`) and returns the surnames of the Nobel Laureates. In other words, the following command should create an RDD with all the surnames. We will use `json.loads` to parse the JSONs (this is already done). Make sure to look at what it returns so you know how to access the information inside the parsed JSONs (these are basically nested dictionaries). (https://docs.python.org/2/library/json.html)'),\n",
       " Row(value='```'),\n",
       " Row(value='     \\ttask7_result = nobelRDD.map(json.loads).flatMap(task2_flatmap)'),\n",
       " Row(value='```'),\n",
       " Row(value=''),\n",
       " Row(value='- **Task 8**: Write a sequence of transformations starting from prizeRDD that returns an PairRDD where the key is the `category` (`physics` etc), and the value is a list of all Nobel Laureates for that category (just their surnames). Make sure the final values are `list`s, and not some other class objects (if you do a `take(5)`, it should print out the lists).'),\n",
       " Row(value=''),\n",
       " Row(value='- **Task 9**: This function operates on the `logsRDD`. It takes as input a list of *dates* and returns an RDD with \"hosts\" that were present in the log on all of '),\n",
       " Row(value=\"those dates. The dates would be provided as strings, in the same format that they appear in the logs (e.g., '01/Jul/1995' and '02/Jul/1995').\"),\n",
       " Row(value='The format of the log entries should be self-explanatory, but here are more details if you need: [NASA Logs](http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html)'),\n",
       " Row(value='Try to minimize the number of RDDs you end up creating.'),\n",
       " Row(value=''),\n",
       " Row(value='- **Task 10**: Complete a function to calculate the degree distribution of user nodes in the Amazon graph (i.e., `amazonBipartiteRDD`). In other words, calculate the degree of each user node (i.e., number of products each user has rated), and then use a reduceByKey (or aggregateByKey) to find the number of nodes with a given degree. The output should be a PairRDD where the key is the degree, and the value is the number of nodes in the graph with that degree.'),\n",
       " Row(value=''),\n",
       " Row(value='### Sample results.txt File'),\n",
       " Row(value='You can use `spark-submit` to run the `spark_assignment.py` file, but it would be easier to develop with `pyspark` (by copying the commands over). '),\n",
       " Row(value=''),\n",
       " Row(value='**results.txt** shows the results of running `spark_assignment.py` on our code using: `$SPARKHOME/bin/spark-submit assignment.py`')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "022c886e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-20T15:42:12.874890Z",
     "start_time": "2022-11-20T15:42:12.695553Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(value='# Assignment 3: Apache Spark')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8cb3cbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-20T15:43:50.912960Z",
     "start_time": "2022-11-20T15:43:50.635484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "linesWithSpark = textFile.filter(textFile.value.contains(\"Spark\"))\n",
    "print(linesWithSpark.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc63fb4",
   "metadata": {},
   "source": [
    "## More on Dataset Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81ba2db5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-20T15:49:27.694243Z",
     "start_time": "2022-11-20T15:49:27.503062Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|size(split(value, \\s+, -1))|\n",
      "+---------------------------+\n",
      "|                          5|\n",
      "|                          1|\n",
      "|                         50|\n",
      "|                          1|\n",
      "|                          5|\n",
      "|                          1|\n",
      "|                         18|\n",
      "|                          1|\n",
      "|                         87|\n",
      "|                          1|\n",
      "|                         19|\n",
      "|                          1|\n",
      "|                          3|\n",
      "|                          1|\n",
      "|                         17|\n",
      "|                          1|\n",
      "|                         18|\n",
      "|                         20|\n",
      "|                          3|\n",
      "|                          9|\n",
      "+---------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Split each line in words and count.\n",
    "df = textFile.select(size(split(textFile.value, \"\\s+\")))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ae2746c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-20T15:49:37.073790Z",
     "start_time": "2022-11-20T15:49:36.922460Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|numWords|\n",
      "+--------+\n",
      "|       5|\n",
      "|       1|\n",
      "|      50|\n",
      "|       1|\n",
      "|       5|\n",
      "|       1|\n",
      "|      18|\n",
      "|       1|\n",
      "|      87|\n",
      "|       1|\n",
      "|      19|\n",
      "|       1|\n",
      "|       3|\n",
      "|       1|\n",
      "|      17|\n",
      "|       1|\n",
      "|      18|\n",
      "|      20|\n",
      "|       3|\n",
      "|       9|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = textFile.select(size(split(textFile.value, \"\\s+\")).name(\"numWords\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56dfdf66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-20T15:49:39.563538Z",
     "start_time": "2022-11-20T15:49:39.348531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(numWords)=101)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the max.\n",
    "df.agg(max(col(\"numWords\"))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de4079d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-20T15:51:28.217262Z",
     "start_time": "2022-11-20T15:51:27.285813Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                word|count|\n",
      "+--------------------+-----+\n",
      "|                some|    3|\n",
      "|                 few|    1|\n",
      "|               input|    5|\n",
      "|            `(user1,|    1|\n",
      "|               those|    1|\n",
      "|   self-explanatory,|    1|\n",
      "|              [Spark|    2|\n",
      "|                 map|    1|\n",
      "|        Messenger]`.|    1|\n",
      "|               Nobel|    2|\n",
      "|website](http://s...|    1|\n",
      "|           typically|    1|\n",
      "|           sanitized|    1|\n",
      "|               ready|    1|\n",
      "|                port|    1|\n",
      "|        manipulation|    1|\n",
      "|         interfaces.|    1|\n",
      "|                  If|    1|\n",
      "|           `pyspark`|    1|\n",
      "|                used|    2|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Implement map-reduce in one line.\n",
    "wordCounts = textFile.select(explode(split(textFile.value, \"\\s+\")).alias(\"word\")).groupBy(\"word\").count()\n",
    "wordCounts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "65d1b1c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-20T15:54:29.306401Z",
     "start_time": "2022-11-20T15:54:29.196683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|       word|\n",
      "+-----------+\n",
      "|          #|\n",
      "| Assignment|\n",
      "|         3:|\n",
      "|     Apache|\n",
      "|      Spark|\n",
      "|           |\n",
      "|        The|\n",
      "|       goal|\n",
      "|         of|\n",
      "|       this|\n",
      "| assignment|\n",
      "|         is|\n",
      "|         to|\n",
      "|      learn|\n",
      "|        how|\n",
      "|         to|\n",
      "|         do|\n",
      "|large-scale|\n",
      "|       data|\n",
      "|   analysis|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert a dataset of lines into a dataset of words.\n",
    "map_ = textFile.select(explode(split(textFile.value, \"\\s+\")).alias(\"word\"))\n",
    "map_.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d5f4b31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-20T15:56:14.635656Z",
     "start_time": "2022-11-20T15:56:13.566691Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(word='some', count=3),\n",
       " Row(word='few', count=1),\n",
       " Row(word='input', count=5),\n",
       " Row(word='`(user1,', count=1),\n",
       " Row(word='those', count=1),\n",
       " Row(word='self-explanatory,', count=1),\n",
       " Row(word='[Spark', count=2),\n",
       " Row(word='map', count=1),\n",
       " Row(word='Messenger]`.', count=1),\n",
       " Row(word='Nobel', count=2),\n",
       " Row(word='website](http://spark.apache.org).', count=1),\n",
       " Row(word='typically', count=1),\n",
       " Row(word='sanitized', count=1),\n",
       " Row(word='ready', count=1),\n",
       " Row(word='port', count=1),\n",
       " Row(word='manipulation', count=1),\n",
       " Row(word='interfaces.', count=1),\n",
       " Row(word='If', count=1),\n",
       " Row(word='`pyspark`', count=1),\n",
       " Row(word='used', count=2),\n",
       " Row(word='basically', count=3),\n",
       " Row(word='Application', count=2),\n",
       " Row(word='Spark](https://spark.apache.org)', count=1),\n",
       " Row(word='(we', count=1),\n",
       " Row(word='local', count=2),\n",
       " Row(word='returns', count=4),\n",
       " Row(word='present', count=1),\n",
       " Row(word='`spark_assignment.py`', count=2),\n",
       " Row(word='assignment,', count=1),\n",
       " Row(word='consisting', count=4),\n",
       " Row(word='within', count=1),\n",
       " Row(word='=', count=3),\n",
       " Row(word='set', count=1),\n",
       " Row(word='line:', count=1),\n",
       " Row(word='`amazonBipartiteRDD`).', count=1),\n",
       " Row(word='SPARKHOME', count=1),\n",
       " Row(word='appears', count=1),\n",
       " Row(word='reading', count=1),\n",
       " Row(word='starting', count=1),\n",
       " Row(word='assignment', count=1),\n",
       " Row(word='class,', count=1),\n",
       " Row(word='originally', count=1),\n",
       " Row(word='`map`', count=3),\n",
       " Row(word='documents', count=1),\n",
       " Row(word='1,', count=1),\n",
       " Row(word='Easiest', count=1),\n",
       " Row(word='`logsRDD`,', count=1),\n",
       " Row(word='Make', count=2),\n",
       " Row(word='not', count=1),\n",
       " Row(word='programs', count=1),\n",
       " Row(word='--port=8881\"', count=1),\n",
       " Row(word='2-tuple,', count=1),\n",
       " Row(word='will', count=10),\n",
       " Row(word='instructions', count=1),\n",
       " Row(word='code', count=5),\n",
       " Row(word='document', count=1),\n",
       " Row(word='sure', count=3),\n",
       " Row(word='form', count=2),\n",
       " Row(word='(i.e.,', count=3),\n",
       " Row(word='relevant', count=1),\n",
       " Row(word='format', count=2),\n",
       " Row(word='by', count=10),\n",
       " Row(word='aggregate.', count=1),\n",
       " Row(word='must', count=1),\n",
       " Row(word='using', count=2),\n",
       " Row(word='b:', count=1),\n",
       " Row(word='records', count=1),\n",
       " Row(word='done).', count=1),\n",
       " Row(word='access', count=1),\n",
       " Row(word='new', count=3),\n",
       " Row(word='based', count=1),\n",
       " Row(word='you', count=12),\n",
       " Row(word='stuff', count=1),\n",
       " Row(word='counts', count=3),\n",
       " Row(word='variety', count=1),\n",
       " Row(word='8', count=1),\n",
       " Row(word='else).', count=1),\n",
       " Row(word='Pre-built', count=1),\n",
       " Row(word='more', count=3),\n",
       " Row(word='In', count=2),\n",
       " Row(word='2-stage', count=1),\n",
       " Row(word='collection', count=1),\n",
       " Row(word='3.', count=2),\n",
       " Row(word='discuss', count=1),\n",
       " Row(word='years,', count=1),\n",
       " Row(word='nobelRDD.map(json.loads).flatMap(task2_flatmap)', count=1),\n",
       " Row(word='3:', count=1),\n",
       " Row(word='PYSPARK_DRIVER_PYTHON_OPTS=\"notebook', count=1),\n",
       " Row(word='+', count=2),\n",
       " Row(word='compact', count=1),\n",
       " Row(word='log', count=4),\n",
       " Row(word='lines)', count=1),\n",
       " Row(word='there', count=1),\n",
       " Row(word='wordcount.py`', count=1),\n",
       " Row(word='Complete', count=3),\n",
       " Row(word='can', count=13),\n",
       " Row(word='`mode`.', count=1),\n",
       " Row(word='*', count=4),\n",
       " Row(word='was', count=1),\n",
       " Row(word='their', count=1),\n",
       " Row(word='for', count=15),\n",
       " Row(word='RDD,', count=2),\n",
       " Row(word='Use', count=1),\n",
       " Row(word='counting', count=1),\n",
       " Row(word='how', count=2),\n",
       " Row(word='1)', count=1),\n",
       " Row(word='Instead', count=1),\n",
       " Row(word='product1', count=1),\n",
       " Row(word='\"hosts\"', count=1),\n",
       " Row(word='computes', count=3),\n",
       " Row(word='dates.', count=1),\n",
       " Row(word='(or', count=1),\n",
       " Row(word='provides', count=1),\n",
       " Row(word='system', count=1),\n",
       " Row(word='(`play.txt`)', count=1),\n",
       " Row(word='develop', count=1),\n",
       " Row(word='languages.', count=1),\n",
       " Row(word='below', count=1),\n",
       " Row(word='PYSPARK_DRIVER_PYTHON=\"jupyter\"', count=1),\n",
       " Row(word='per', count=1),\n",
       " Row(word='`reduce`', count=1),\n",
       " Row(word='filter', count=1),\n",
       " Row(word='guide', count=2),\n",
       " Row(word='directly.', count=1),\n",
       " Row(word='words', count=3),\n",
       " Row(word='parse', count=1),\n",
       " Row(word='system);', count=1),\n",
       " Row(word='surnames', count=1),\n",
       " Row(word='one', count=2),\n",
       " Row(word='preferable,', count=1),\n",
       " Row(word='program', count=1),\n",
       " Row(word='1**:', count=1),\n",
       " Row(word='in', count=20),\n",
       " Row(word='UC', count=1),\n",
       " Row(word='--allow-root', count=1),\n",
       " Row(word='(`NASA_logs_sample.txt`)', count=1),\n",
       " Row(word='open-source', count=1),\n",
       " Row(word='Move', count=1),\n",
       " Row(word='Spark.', count=1),\n",
       " Row(word='average', count=1),\n",
       " Row(word='`aggregateByKey`', count=1),\n",
       " Row(word='year.', count=1),\n",
       " Row(word='application.', count=1),\n",
       " Row(word='splits', count=1),\n",
       " Row(word='contains', count=2),\n",
       " Row(word='they', count=2),\n",
       " Row(word='5', count=1),\n",
       " Row(word='with', count=12),\n",
       " Row(word='Your', count=1),\n",
       " Row(word='teh', count=1),\n",
       " Row(word='items,', count=1),\n",
       " Row(word='downloaded', count=2),\n",
       " Row(word='--no-browser', count=1),\n",
       " Row(word='$SPARKHOME/bin/pyspark', count=1),\n",
       " Row(word='mapping', count=1),\n",
       " Row(word='group', count=1),\n",
       " Row(word='count', count=1),\n",
       " Row(word='surnames.', count=1),\n",
       " Row(word='given', count=1),\n",
       " Row(word='Write', count=4),\n",
       " Row(word='5.0\"', count=1),\n",
       " Row(word='(these', count=1),\n",
       " Row(word='large,', count=1),\n",
       " Row(word='out', count=4),\n",
       " Row(word='takes', count=6),\n",
       " Row(word='3.3', count=1),\n",
       " Row(word='####', count=1),\n",
       " Row(word='characters.', count=1),\n",
       " Row(word='be', count=17),\n",
       " Row(word='same', count=3),\n",
       " Row(word='RDD', count=15),\n",
       " Row(word='So', count=2),\n",
       " Row(word='`spark-3.2.0-bin-hadoop3.2/`.', count=1),\n",
       " Row(word='already', count=1),\n",
       " Row(word='9**:', count=1),\n",
       " Row(word='(RDDs)**.', count=1),\n",
       " Row(word='textFile', count=1),\n",
       " Row(word='`reduceByKey`', count=1),\n",
       " Row(word='sc.textFile(\"README.md\")`:', count=1),\n",
       " Row(word='extract', count=1),\n",
       " Row(word='>10).', count=1),\n",
       " Row(word='\"user1', count=1),\n",
       " Row(word='task7_result', count=1),\n",
       " Row(word='distributed', count=3),\n",
       " Row(word='mode;', count=1),\n",
       " Row(word='these', count=2),\n",
       " Row(word='lines', count=3),\n",
       " Row(word='`(1,', count=1),\n",
       " Row(word='created', count=1),\n",
       " Row(word='using:', count=2),\n",
       " Row(word='output.', count=1),\n",
       " Row(word='aggregateByKey)', count=1),\n",
       " Row(word='[Apache', count=1),\n",
       " Row(word='instead', count=1),\n",
       " Row(word='b):', count=1),\n",
       " Row(word='your', count=1),\n",
       " Row(word='inside', count=1),\n",
       " Row(word='ties,', count=1),\n",
       " Row(word='Spark:', count=1),\n",
       " Row(word='create', count=3),\n",
       " Row(word='should', count=8),\n",
       " Row(word='years', count=1),\n",
       " Row(word='package', count=1),\n",
       " Row(word='line.split(\"', count=2),\n",
       " Row(word='\")).map(lambda', count=1),\n",
       " Row(word='Programming', count=1),\n",
       " Row(word='one.', count=1),\n",
       " Row(word='graph', count=2),\n",
       " Row(word='commands', count=3),\n",
       " Row(word=\"'01/Jul/1995'\", count=1),\n",
       " Row(word='(http://spark.apache.org/docs/latest/quick-start.html)', count=1),\n",
       " Row(word='appropriately', count=1),\n",
       " Row(word='nodes', count=3),\n",
       " Row(word='resource', count=1),\n",
       " Row(word='Download', count=1),\n",
       " Row(word='somewhere', count=1),\n",
       " Row(word='requests', count=1),\n",
       " Row(word='variables', count=1),\n",
       " Row(word='file,', count=2),\n",
       " Row(word=\"'02/Jul/1995').\", count=1),\n",
       " Row(word='however', count=1),\n",
       " Row(word='doing).', count=1),\n",
       " Row(word='count,', count=1),\n",
       " Row(word='**Task', count=10),\n",
       " Row(word='descriptor,', count=1),\n",
       " Row(word='other', count=5),\n",
       " Row(word='times', count=1),\n",
       " Row(word='file:', count=1),\n",
       " Row(word='easier', count=1),\n",
       " Row(word='popularized', count=1),\n",
       " Row(word='ecosystem,', count=1),\n",
       " Row(word='otherwise', count=1),\n",
       " Row(word='line,', count=5),\n",
       " Row(word='do:', count=2),\n",
       " Row(word='BEATRICE,', count=1),\n",
       " Row(word='logs', count=1),\n",
       " Row(word='RDDs:', count=1),\n",
       " Row(word='prizeRDD', count=1),\n",
       " Row(word='Apache', count=2),\n",
       " Row(word='etc),', count=1),\n",
       " Row(word='download', count=1),\n",
       " Row(word='`export', count=1),\n",
       " Row(word='detailed', count=1),\n",
       " Row(word='7**:', count=1),\n",
       " Row(word='is', count=25),\n",
       " Row(word='tutorial', count=1),\n",
       " Row(word='on', count=7),\n",
       " Row(word='Getting', count=1),\n",
       " Row(word='user', count=4),\n",
       " Row(word='sequence', count=1),\n",
       " Row(word='(Spark', count=1),\n",
       " Row(word='verbose', count=1),\n",
       " Row(word='8881', count=1),\n",
       " Row(word='but', count=5),\n",
       " Row(word='initializes', count=1),\n",
       " Row(word='`prize.json`)', count=1),\n",
       " Row(word='without', count=1),\n",
       " Row(word='selecting', count=1),\n",
       " Row(word='10.', count=1),\n",
       " Row(word='strings,', count=1),\n",
       " Row(word='end', count=1),\n",
       " Row(word='degree.', count=2),\n",
       " Row(word='(by', count=1),\n",
       " Row(word='uncompress', count=1),\n",
       " Row(word='`>>>', count=2),\n",
       " Row(word='each', count=10),\n",
       " Row(word='operations.', count=1),\n",
       " Row(word='(http://spark.apache.org/docs/latest/quick-start.html).', count=1),\n",
       " Row(word='playRDD', count=1),\n",
       " Row(word='values', count=1),\n",
       " Row(word='print', count=1),\n",
       " Row(word='creating.', count=1),\n",
       " Row(word='directory:', count=1),\n",
       " Row(word='Set', count=1),\n",
       " Row(word='creates', count=1),\n",
       " Row(word='entry', count=1),\n",
       " Row(word='gets', count=1),\n",
       " Row(word='remove', count=1),\n",
       " Row(word='goal', count=1),\n",
       " Row(word='use', count=12),\n",
       " Row(word='Sample', count=1),\n",
       " Row(word='File', count=1),\n",
       " Row(word='into', count=2),\n",
       " Row(word='`textFile.first()`,', count=1),\n",
       " Row(word='following', count=3),\n",
       " Row(word='any', count=1),\n",
       " Row(word='provided),', count=1),\n",
       " Row(word='functions,', count=2),\n",
       " Row(word='data', count=2),\n",
       " Row(word='it', count=13),\n",
       " Row(word='analysis', count=2),\n",
       " Row(word='runs', count=1),\n",
       " Row(word='`mode`', count=1),\n",
       " Row(word='`groupByKey`', count=1),\n",
       " Row(word='`wordcount.py`,', count=1),\n",
       " Row(word='probably', count=1),\n",
       " Row(word='does', count=1),\n",
       " Row(word=\"won't\", count=1),\n",
       " Row(word='developed', count=1),\n",
       " Row(word='primarily', count=1),\n",
       " Row(word='have', count=2),\n",
       " Row(word='(in', count=1),\n",
       " Row(word='-', count=10),\n",
       " Row(word='only', count=1),\n",
       " Row(word='doing', count=1),\n",
       " Row(word='a,', count=2),\n",
       " Row(word='[Hadoop', count=1),\n",
       " Row(word='results.txt', count=1),\n",
       " Row(word='You', count=6),\n",
       " Row(word='`$SPARKHOME/bin/pyspark`:', count=1),\n",
       " Row(word='(if', count=1),\n",
       " Row(word='items', count=1),\n",
       " Row(word='Noble', count=1),\n",
       " Row(word='It', count=3),\n",
       " Row(word='(which', count=3),\n",
       " Row(word='(it', count=1),\n",
       " Row(word='need:', count=1),\n",
       " Row(word='our', count=1),\n",
       " Row(word='spark', count=1),\n",
       " Row(word='second', count=1),\n",
       " Row(word='(https://docs.python.org/2/library/json.html)', count=1),\n",
       " Row(word='appear', count=1),\n",
       " Row(word='array', count=1),\n",
       " Row(word='description:', count=1),\n",
       " Row(word='`spark-submit`', count=1),\n",
       " Row(word=\"'/data/Assignment-3'),\", count=1),\n",
       " Row(word='just', count=4),\n",
       " Row(word='while', count=1),\n",
       " Row(word='done', count=1),\n",
       " Row(word='file', count=7),\n",
       " Row(word='rest', count=1),\n",
       " Row(word='pyspark', count=1),\n",
       " Row(word='non-alphanumerical', count=1),\n",
       " Row(word='(e.g.,', count=1),\n",
       " Row(word='the', count=121),\n",
       " Row(word='Details', count=1),\n",
       " Row(word='across', count=2),\n",
       " Row(word='find', count=1),\n",
       " Row(word='To', count=2),\n",
       " Row(word='commands.', count=1),\n",
       " Row(word='ratings', count=1),\n",
       " Row(word='[NASA', count=1),\n",
       " Row(word='(and', count=1),\n",
       " Row(word='write', count=3),\n",
       " Row(word='function', count=8),\n",
       " Row(word='initial', count=1),\n",
       " Row(word='anything', count=1),\n",
       " Row(word='ask', count=1),\n",
       " Row(word='`README.md`.', count=1),\n",
       " Row(word='see', count=2),\n",
       " Row(word='`amazonInputRDD`', count=3),\n",
       " Row(word='correct', count=2),\n",
       " Row(word='followed', count=4),\n",
       " Row(word='`spark_assignment.py`,', count=1),\n",
       " Row(word='Dataset', count=1),\n",
       " Row(word='return', count=3),\n",
       " Row(word='separate', count=1),\n",
       " Row(word='5**:', count=1),\n",
       " Row(word='copying', count=1),\n",
       " Row(word='computing', count=1),\n",
       " Row(word='Java,', count=1),\n",
       " Row(word='simply', count=1),\n",
       " Row(word='word', count=4),\n",
       " Row(word='2-tuples', count=2),\n",
       " Row(word='framework,', count=1),\n",
       " Row(word='2952)`', count=1),\n",
       " Row(word='excellent', count=2),\n",
       " Row(word='from', count=7),\n",
       " Row(word='definitions.', count=1),\n",
       " Row(word='LEONATO,', count=1),\n",
       " Row(word='parsed', count=2),\n",
       " Row(word='*dates*', count=1),\n",
       " Row(word='cluster', count=1),\n",
       " Row(word='output', count=7),\n",
       " Row(word='shell.', count=2),\n",
       " Row(word='`filter`.', count=1),\n",
       " Row(word='ways.', count=1),\n",
       " Row(word='shell', count=1),\n",
       " Row(word='containing', count=2),\n",
       " Row(word='`logsRDD`.', count=1),\n",
       " Row(word='much', count=1),\n",
       " Row(word='datasets', count=2),\n",
       " Row(word='tutorials', count=1),\n",
       " Row(word='Google', count=1),\n",
       " Row(word='hard', count=1),\n",
       " Row(word='make', count=1),\n",
       " Row(word='`lambda`', count=2),\n",
       " Row(word='Running', count=1),\n",
       " Row(word='cluster.', count=1),\n",
       " Row(word='encourage', count=1),\n",
       " Row(word='compute', count=1),\n",
       " Row(word='degree', count=2),\n",
       " Row(word='what', count=2),\n",
       " Row(word='\")', count=1),\n",
       " Row(word='defined', count=1),\n",
       " Row(word='words.', count=1),\n",
       " Row(word='fill', count=1),\n",
       " Row(word='follow', count=2),\n",
       " Row(word='(word,', count=2),\n",
       " Row(word='list', count=2),\n",
       " Row(word='details', count=1),\n",
       " Row(word='up', count=1),\n",
       " Row(word='textFile.flatMap(lambda', count=1),\n",
       " Row(word='Try', count=1),\n",
       " Row(word='over).', count=1),\n",
       " Row(word='(`amazon-ratings.txt`)', count=1),\n",
       " Row(word='follow.', count=1),\n",
       " Row(word='provided', count=2),\n",
       " Row(word='single', count=1),\n",
       " Row(word='generalizes', count=1),\n",
       " Row(word='Scala),', count=1),\n",
       " Row(word='shell)', count=1),\n",
       " Row(word='higher', count=1),\n",
       " Row(word='and', count=30),\n",
       " Row(word='Started', count=1),\n",
       " Row(word='chains', count=1),\n",
       " Row(word='do', count=6),\n",
       " Row(word='Scala', count=2),\n",
       " Row(word='`task`).', count=1),\n",
       " Row(word='class', count=1),\n",
       " Row(word='operations', count=2),\n",
       " Row(word='SPARKHOME=/data/Assignment-3/spark-3.2.0-bin-hadoop3.2/`', count=1),\n",
       " Row(word='here', count=3),\n",
       " Row(word='command', count=2),\n",
       " Row(word='three', count=1),\n",
       " Row(word='PYSPARK_PYTHON=/usr/bin/python3', count=1),\n",
       " Row(word='Laureates', count=2),\n",
       " Row(word='operates', count=2),\n",
       " Row(word='objects', count=1),\n",
       " Row(word='Berkeley.', count=1),\n",
       " Row(word='proposed', count=1),\n",
       " Row(word='need', count=1),\n",
       " Row(word='amount', count=1),\n",
       " Row(word='are', count=10),\n",
       " Row(word='requires', count=1),\n",
       " Row(word='variable:', count=1),\n",
       " Row(word='b)`', count=1),\n",
       " Row(word='look', count=2),\n",
       " Row(word='where', count=4),\n",
       " Row(word='abstraction', count=1),\n",
       " Row(word='final', count=1),\n",
       " Row(word='large-scale', count=1),\n",
       " Row(word='summary', count=1),\n",
       " Row(word='languages:', count=1),\n",
       " Row(word='`textFile.count()`', count=1),\n",
       " Row(word='better', count=1),\n",
       " Row(word='8**:', count=1),\n",
       " Row(word='`take(5)`,', count=1),\n",
       " Row(word='small', count=3),\n",
       " Row(word='datasets.', count=1),\n",
       " Row(word='**Version', count=1),\n",
       " Row(word='entries', count=1),\n",
       " Row(word='of', count=40),\n",
       " Row(word='http://spark.apache.org/downloads.html.', count=1),\n",
       " Row(word='through', count=2),\n",
       " Row(word='Walk-Through).', count=1),\n",
       " Row(word='Laureates.', count=1),\n",
       " Row(word='`Assignment-3/`', count=1),\n",
       " Row(word='very', count=1),\n",
       " Row(word='i.e.,', count=2),\n",
       " Row(word='`$SPARKHOME/bin/spark-submit', count=2),\n",
       " Row(word='5.0)`.', count=1),\n",
       " Row(word='3**:', count=1),\n",
       " Row(word='paradigm', count=1),\n",
       " Row(word='`textFile`,', count=1),\n",
       " Row(word='(starting', count=1),\n",
       " Row(word='Notebook', count=3),\n",
       " Row(word='Shakespeare', count=1),\n",
       " Row(word='category', count=1),\n",
       " Row(word='surnames).', count=1),\n",
       " Row(word='reduceByKey', count=1),\n",
       " Row(word='The', count=16),\n",
       " Row(word='significantly', count=1),\n",
       " Row(word='rating.', count=1),\n",
       " Row(word='6**:', count=1),\n",
       " Row(word='PySpark', count=2),\n",
       " Row(word='(this', count=1),\n",
       " Row(word='YARN', count=1),\n",
       " Row(word='words,', count=3),\n",
       " Row(word='examples', count=1),\n",
       " Row(word='`[Enter,', count=1),\n",
       " Row(word='lists).', count=1),\n",
       " Row(word='```', count=6),\n",
       " Row(word='JSON', count=2),\n",
       " Row(word='an', count=12),\n",
       " Row(word='initialized', count=1),\n",
       " Row(word='over', count=1),\n",
       " Row(word='2.87)`', count=1),\n",
       " Row(word='#', count=1),\n",
       " Row(word='including', count=1),\n",
       " Row(word='common', count=1),\n",
       " Row(word='###', count=9),\n",
       " Row(word='found', count=1),\n",
       " Row(word='Python', count=7),\n",
       " Row(word='quick', count=2),\n",
       " Row(word='RDD).', count=1),\n",
       " Row(word='users', count=1),\n",
       " Row(word='key', count=3),\n",
       " Row(word='at', count=6),\n",
       " Row(word='interface', count=1),\n",
       " Row(word='Here', count=2),\n",
       " Row(word='RDDs.', count=1),\n",
       " Row(word='dictionaries).', count=1),\n",
       " Row(word='website.', count=1),\n",
       " Row(word='mode.', count=1),\n",
       " Row(word='4**:', count=1),\n",
       " Row(word='finds', count=1),\n",
       " Row(word='tasks', count=3),\n",
       " Row(word='**resilient', count=1),\n",
       " Row(word='way', count=3),\n",
       " Row(word='try', count=1),\n",
       " Row(word='PairRDD', count=2),\n",
       " Row(word='(just', count=1),\n",
       " Row(word='10**:', count=1),\n",
       " Row(word='that', count=20),\n",
       " Row(word='Tutorial](http://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html#Source+Code)', count=1),\n",
       " Row(word='For', count=1),\n",
       " Row(word='list:', count=1),\n",
       " Row(word='dates', count=1),\n",
       " Row(word='manager.', count=1),\n",
       " Row(word='prints', count=1),\n",
       " Row(word='b', count=1),\n",
       " Row(word='outputs', count=1),\n",
       " Row(word='later**.', count=1),\n",
       " Row(word='--ip=0.0.0.0', count=1),\n",
       " Row(word='were', count=1),\n",
       " Row(word='node', count=1),\n",
       " Row(word='This', count=8),\n",
       " Row(word='(modify', count=1),\n",
       " Row(word='would', count=6),\n",
       " Row(word='`(1995,', count=1),\n",
       " Row(word='results', count=1),\n",
       " Row(word='running', count=1),\n",
       " Row(word='run', count=4),\n",
       " Row(word='larger', count=1),\n",
       " Row(word='2.', count=2),\n",
       " Row(word='split(line):', count=1),\n",
       " Row(word='Spark', count=14),\n",
       " Row(word='Jupyter', count=2),\n",
       " Row(word='product).', count=1),\n",
       " Row(word='easily', count=1),\n",
       " Row(word='sum(a,', count=1),\n",
       " Row(word='representation', count=1),\n",
       " Row(word='`json.loads`', count=1),\n",
       " Row(word='Installing', count=1),\n",
       " Row(word='Since', count=1),\n",
       " Row(word='Python.', count=1),\n",
       " Row(word='standard', count=2),\n",
       " Row(word='shell,', count=2),\n",
       " Row(word='tuple', count=2),\n",
       " Row(word='user-product', count=1),\n",
       " Row(word='all', count=6),\n",
       " Row(word='removing', count=1),\n",
       " Row(word='value', count=3),\n",
       " Row(word='so', count=1),\n",
       " Row(word='transformations', count=1),\n",
       " Row(word='(so', count=1),\n",
       " Row(word='reviewed.', count=1),\n",
       " Row(word='calculate', count=2),\n",
       " Row(word='**results.txt**', count=1),\n",
       " Row(word='a', count=45),\n",
       " Row(word='if', count=2),\n",
       " Row(word='`maps`', count=1),\n",
       " Row(word='most', count=1),\n",
       " Row(word='JSONs', count=2),\n",
       " Row(word='degree,', count=1),\n",
       " Row(word='available', count=1),\n",
       " Row(word='`category`', count=1),\n",
       " Row(word='', count=67),\n",
       " Row(word='as', count=9),\n",
       " Row(word='`flatmap`', count=1),\n",
       " Row(word='especially', count=1),\n",
       " Row(word='(several', count=1),\n",
       " Row(word='either', count=1),\n",
       " Row(word='pick', count=1),\n",
       " Row(word='this', count=6),\n",
       " Row(word='--', count=2),\n",
       " Row(word='play', count=3),\n",
       " Row(word='Guide](https://spark.apache.org/docs/latest/programming-guide.html)', count=1),\n",
       " Row(word='`textFile.take(5)`', count=1),\n",
       " Row(word='being', count=2),\n",
       " Row(word='Simplest', count=1),\n",
       " Row(word='`list`s,', count=1),\n",
       " Row(word='learn', count=1),\n",
       " Row(word='1)).reduceByKey(lambda', count=1),\n",
       " Row(word='`task5_flatmap`', count=1),\n",
       " Row(word='`playRDD`', count=1),\n",
       " Row(word='written', count=2),\n",
       " Row(word='1.', count=2),\n",
       " Row(word='about', count=2),\n",
       " Row(word='spark-3.2.0-bin-hadoop3.2.tgz`', count=1),\n",
       " Row(word='information', count=2),\n",
       " Row(word='Amazon', count=2),\n",
       " Row(word='Java', count=2),\n",
       " Row(word='work.', count=1),\n",
       " Row(word='Word', count=2),\n",
       " Row(word='>', count=1),\n",
       " Row(word='know', count=1),\n",
       " Row(word='3.2.0,', count=1),\n",
       " Row(word='word:', count=1),\n",
       " Row(word='textfile.flatMap(split).map(generateone).reduceByKey(sum)', count=1),\n",
       " Row(word='has', count=1),\n",
       " Row(word='`tar', count=1),\n",
       " Row(word='zxvf', count=1),\n",
       " Row(word='Shell', count=2),\n",
       " Row(word='recommend', count=1),\n",
       " Row(word='then', count=2),\n",
       " Row(word='line', count=6),\n",
       " Row(word='large', count=1),\n",
       " Row(word='products', count=2),\n",
       " Row(word='flatmap', count=2),\n",
       " Row(word='HERO,', count=1),\n",
       " Row(word='number', count=8),\n",
       " Row(word='first', count=4),\n",
       " Row(word='(`prize.json`)', count=1),\n",
       " Row(word='product', count=1),\n",
       " Row(word='An', count=5),\n",
       " Row(word='folllowing', count=1),\n",
       " Row(word='`map`.', count=2),\n",
       " Row(word='and,', count=1),\n",
       " Row(word='HDFS', count=1),\n",
       " Row(word='4.', count=2),\n",
       " Row(word='file.', count=2),\n",
       " Row(word='(`task7_flatmap`)', count=1),\n",
       " Row(word='Logs](http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html)', count=1),\n",
       " Row(word='minimize', count=1),\n",
       " Row(word='Assignment', count=2),\n",
       " Row(word='or', count=5),\n",
       " Row(word='shows', count=2),\n",
       " Row(word='`project5`', count=1),\n",
       " Row(word='More...', count=1),\n",
       " Row(word='functions', count=1),\n",
       " Row(word='2**:', count=1),\n",
       " Row(word='to', count=33),\n",
       " Row(word='directly', count=1),\n",
       " Row(word='generateone(word):', count=1),\n",
       " Row(word='3rd', count=1),\n",
       " Row(word='rated),', count=1),\n",
       " Row(word='relatively', count=2),\n",
       " Row(word='Hadoop', count=3),\n",
       " Row(word='python', count=3),\n",
       " Row(word='def', count=3),\n",
       " Row(word='(look', count=1),\n",
       " Row(word='last', count=2),\n",
       " Row(word='answer).', count=2),\n",
       " Row(word='indicating', count=1),\n",
       " Row(word='mapped', count=1),\n",
       " Row(word='RDDs', count=2),\n",
       " Row(word='equivalent', count=1),\n",
       " Row(word='program,', count=1),\n",
       " Row(word='individual', count=1),\n",
       " Row(word='with,', count=1),\n",
       " Row(word='directory', count=2),\n",
       " Row(word='also', count=6),\n",
       " Row(word='pertaining', count=1),\n",
       " Row(word='we', count=5),\n",
       " Row(word='*submit*', count=1),\n",
       " Row(word='one-liners),', count=1),\n",
       " Row(word='exception', count=1),\n",
       " Row(word='rating', count=3),\n",
       " Row(word='nested', count=1),\n",
       " Row(word='supports', count=1),\n",
       " Row(word='functions.', count=1),\n",
       " Row(word='assignment.py`', count=1),\n",
       " Row(word='transform', count=1),\n",
       " Row(word='We', count=7),\n",
       " Row(word='start', count=3),\n",
       " Row(word='`functions.py`', count=1),\n",
       " Row(word='`counts.take(5)`', count=1),\n",
       " Row(word='which', count=1),\n",
       " Row(word='Map-Reduce', count=2),\n",
       " Row(word='distribution', count=2),\n",
       " Row(word='called', count=1),\n",
       " Row(word='(not', count=2),\n",
       " Row(word='(`physics`', count=1),\n",
       " Row(word='(originally', count=1),\n",
       " Row(word='bunch', count=1),\n",
       " Row(word='`sc.textFile`', count=1),\n",
       " Row(word='Count', count=2),\n",
       " Row(word='(from', count=1)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = map_.groupBy(\"word\").count()\n",
    "\n",
    "#result.show()\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bde11e",
   "metadata": {},
   "source": [
    "# RDD Programming guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b4db553b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T22:10:25.149548Z",
     "start_time": "2022-11-21T22:10:25.097150Z"
    }
   },
   "outputs": [],
   "source": [
    "class DisplayRDD:\n",
    "        def __init__(self, rdd):\n",
    "                self.rdd = rdd\n",
    "\n",
    "        def _repr_html_(self):                                  \n",
    "                x = self.rdd.mapPartitionsWithIndex(lambda i, x: [(i, [y for y in x])])\n",
    "                l = x.collect()\n",
    "                s = \"<table><tr>{}</tr><tr><td>\".format(\"\".join([\"<th>Partition {}\".format(str(j)) for (j, r) in l]))\n",
    "                s += '</td><td valign=\"bottom\" halignt=\"left\">'.join([\"<ul><li>{}</ul>\".format(\"<li>\".join([str(rr) for rr in r])) for (j, r) in l])\n",
    "                s += \"</td></table>\"\n",
    "                return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ddffcf48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T22:13:49.945230Z",
     "start_time": "2022-11-21T22:13:49.532139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[117] at readRDDFromFile at PythonRDD.scala:274\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>Partition 0<th>Partition 1<th>Partition 2<th>Partition 3<th>Partition 4<th>Partition 5</tr><tr><td><ul><li>0<li>1<li>2</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>3<li>4<li>5</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>6<li>7<li>8</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>9<li>10<li>11</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>12<li>13<li>14</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>15<li>16<li>17<li>18<li>19</ul></td></table>"
      ],
      "text/plain": [
       "<__main__.DisplayRDD at 0x7fdcba0eff10>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = list(range(20))\n",
    "\n",
    "data_rdd = sc.parallelize(data)\n",
    "print(data_rdd)\n",
    "\n",
    "DisplayRDD(data_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c1681168",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T22:13:56.780028Z",
     "start_time": "2022-11-21T22:13:56.395230Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[119] at readRDDFromFile at PythonRDD.scala:274\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>Partition 0<th>Partition 1<th>Partition 2<th>Partition 3<th>Partition 4<th>Partition 5<th>Partition 6<th>Partition 7<th>Partition 8<th>Partition 9</tr><tr><td><ul><li>0<li>1</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>2<li>3</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>4<li>5</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>6<li>7</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>8<li>9</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>10<li>11</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>12<li>13</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>14<li>15</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>16<li>17</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>18<li>19</ul></td></table>"
      ],
      "text/plain": [
       "<__main__.DisplayRDD at 0x7fdcba0efca0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = list(range(20))\n",
    "\n",
    "data_rdd = sc.parallelize(data, 10)\n",
    "print(data_rdd)\n",
    "\n",
    "DisplayRDD(data_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a7dfb10b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T22:14:15.020310Z",
     "start_time": "2022-11-21T22:14:14.586532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states.txt MapPartitionsRDD[127] at textFile at NativeMethodAccessorImpl.java:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>Partition 0<th>Partition 1<th>Partition 2<th>Partition 3<th>Partition 4<th>Partition 5<th>Partition 6<th>Partition 7<th>Partition 8<th>Partition 9</tr><tr><td><ul><li>Alabama<li>Hawaii<li>Massachusetts<li>New Mexico<li>South Dakota</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>Alaska<li>Idaho<li>Michigan<li>New York<li>Tennessee<li>Arizona</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>Illinois<li>Minnesota<li>North Carolina<li>Texas</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>Arkansas<li>Indiana<li>Mississippi<li>North Dakota<li>Utah</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>California<li>Iowa<li>Missouri<li>Ohio<li>Vermont<li>Colorado</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>Kansas<li>Montana<li>Oklahoma<li>Virginia<li>Connecticut<li>Kentucky</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>Nebraska<li>Oregon<li>Washington<li>Delaware<li>Louisiana</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>Nevada<li>Pennsylvania<li>West Virginia<li>Florida</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>Maine<li>New Hampshire<li>Rhode Island<li>Wisconsin<li>Georgia</ul></td><td valign=\"bottom\" halignt=\"left\"><ul><li>Maryland<li>New Jersey<li>South Carolina<li>Wyoming</ul></td></table>"
      ],
      "text/plain": [
       "<__main__.DisplayRDD at 0x7fdcba94fe50>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return one record per line.\n",
    "states_rdd = sc.textFile('states.txt', 10)\n",
    "print(states_rdd)\n",
    "DisplayRDD(states_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dae165",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "351d1516",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T23:32:06.811402Z",
     "start_time": "2022-11-21T23:32:06.606785Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422\n"
     ]
    }
   ],
   "source": [
    "# lines and lineLengths are not computed immediately (due to lazy execution).\n",
    "lines = sc.textFile(\"states.txt\")\n",
    "lineLengths = lines.map(lambda s: len(s))\n",
    "# reduce is an aciton and triggers the execution.\n",
    "totalLength = lineLengths.reduce(lambda a, b: a + b)\n",
    "print(totalLength)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32598dd9",
   "metadata": {},
   "source": [
    "## Passing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "963012b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T23:34:04.687099Z",
     "start_time": "2022-11-21T23:34:04.445239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "def myFunc(s):\n",
    "    words = s.split(\" \")\n",
    "    return len(words)\n",
    "\n",
    "\n",
    "lines = sc.textFile(\"states.txt\")\n",
    "lineLengths = lines.map(lambda s: myFunc(s))\n",
    "# reduce is an aciton and triggers the execution.\n",
    "totalLength = lineLengths.reduce(lambda a, b: a + b)\n",
    "print(totalLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "be873b66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T23:37:21.901148Z",
     "start_time": "2022-11-21T23:37:21.677603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "Counter value:  0\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "print(data)\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Wrong: Don't do this!!\n",
    "def increment_counter(x):\n",
    "    global counter\n",
    "    counter += x\n",
    "rdd.foreach(increment_counter)\n",
    "\n",
    "# The output is zero since the executors are updating the copy.\n",
    "print(\"Counter value: \", counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "79c94a69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T23:45:19.849469Z",
     "start_time": "2022-11-21T23:45:19.362072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Tennessee', 2), ('Arizona', 2), ('North Dakota', 2), ('Nebraska', 2), ('Washington', 2), ('West Virginia', 2), ('New Hampshire', 2), ('Maryland', 2), ('New Jersey', 2), ('South Carolina', 2), ('Alabama', 2), ('Massachusetts', 2), ('Michigan', 2), ('Mississippi', 2), ('Utah', 2), ('Iowa', 2), ('Missouri', 2), ('Ohio', 2), ('Montana', 2), ('Connecticut', 2), ('Kentucky', 2), ('Nevada', 2), ('Rhode Island', 2), ('Georgia', 2), ('Hawaii', 2), ('New Mexico', 2), ('Illinois', 2), ('Minnesota', 2), ('North Carolina', 2), ('Texas', 2), ('Arkansas', 2), ('Indiana', 2), ('Vermont', 2), ('Colorado', 2), ('Kansas', 2), ('Oregon', 2), ('Delaware', 2), ('Louisiana', 2), ('Florida', 2), ('Maine', 2), ('South Dakota', 2), ('Alaska', 2), ('Idaho', 2), ('New York', 2), ('California', 2), ('Oklahoma', 2), ('Virginia', 2), ('Pennsylvania', 2), ('Wisconsin', 2), ('Wyoming', 2)]\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(\"states.txt\") + sc.textFile(\"states.txt\")\n",
    "pairs = lines.map(lambda s: (s, 1))\n",
    "counts = pairs.reduceByKey(lambda a, b: a + b)\n",
    "print(counts.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d7a8e7",
   "metadata": {},
   "source": [
    "## Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d7cf6262",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-23T09:27:53.843847Z",
     "start_time": "2022-11-23T09:27:53.457425Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi is roughly 3.141400\n"
     ]
    }
   ],
   "source": [
    "# Estimate π (compute-intensive task).\n",
    "# Pick random points in the unit square [(0,0)-(1,1)].\n",
    "# See how many fall in the unit circle center=(0, 0), radius=1.\n",
    "# The fraction should be π / 4.\n",
    "\n",
    "import random\n",
    "random.seed(314)\n",
    "\n",
    "def sample(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    in_unit_circle = 1 if x*x + y*y < 1 else 0\n",
    "    return in_unit_circle\n",
    "\n",
    "# “parallelize” method creates an RDD.\n",
    "NUM_SAMPLES = int(1e6)\n",
    "count = sc.parallelize(range(0, NUM_SAMPLES)) \\\n",
    "           .map(sample) \\\n",
    "           .reduce(lambda a, b: a + b)\n",
    "approx_pi = 4.0 * count / NUM_SAMPLES\n",
    "print(\"pi is roughly %f\" % approx_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9644bfdb",
   "metadata": {},
   "source": [
    "## Working with key-value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8c924af4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-23T09:37:35.515814Z",
     "start_time": "2022-11-23T09:37:33.744817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One a penny, two a penny, hot cross buns\r\n"
     ]
    }
   ],
   "source": [
    "!more data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3a36105a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-23T09:36:24.809118Z",
     "start_time": "2022-11-23T09:36:24.381059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('One', 1), ('two', 1), ('hot', 1), ('cross', 1), ('a', 2), ('penny,', 2), ('buns', 1)]\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(\"data.txt\").flatMap(lambda line: line.split(\" \"))\n",
    "pairs = lines.map(lambda s: (s, 1))\n",
    "counts = pairs.reduceByKey(lambda a, b: a + b)\n",
    "result = counts.collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6e3fba51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-23T10:06:00.567448Z",
     "start_time": "2022-11-23T10:05:59.976231Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('One', 1), ('two', 1), ('hot', 1), ('cross', 1), ('a', 2), ('penny,', 2), ('buns', 1)]\n"
     ]
    }
   ],
   "source": [
    "result = sc.textFile(\"data.txt\").\\\n",
    "    flatMap(lambda line: line.split(\" \")).\\\n",
    "    map(lambda s: (s, 1))\\\n",
    "    .reduceByKey(lambda a, b: a + b)\\\n",
    "#     .collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1796b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142710b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8e6c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shuffle operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9250b722",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RDD persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef388e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shared variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4d9e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33187e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2b3aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb74ea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GraphX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fe4b8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T01:07:44.272453Z",
     "start_time": "2022-11-25T01:07:44.066071Z"
    }
   },
   "source": [
    "# pyspark\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/getting_started/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0df2a06e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T01:09:51.675106Z",
     "start_time": "2022-11-25T01:09:26.262369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/lib/python3/dist-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Collecting numpy>=1.21.0\n",
      "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, pandas\n",
      "Successfully installed numpy-1.23.5 pandas-1.5.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0b3d238e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T01:08:37.440560Z",
     "start_time": "2022-11-25T01:08:37.418377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7fdce815d840>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "531b88b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T01:09:57.035421Z",
     "start_time": "2022-11-25T01:09:51.678216Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1,\n",
    "        b=2.,\n",
    "        c='string1',\n",
    "        d=date(2000, 1, 1),\n",
    "        e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2,\n",
    "        b=3.,\n",
    "        c='string2',\n",
    "        d=date(2000, 2, 1),\n",
    "        e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4,\n",
    "        b=5.,\n",
    "        c='string3',\n",
    "        d=date(2000, 3, 1),\n",
    "        e=datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
